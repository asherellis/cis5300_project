{
  "rag_extension6": {
    "retrieval": {
      "recall_at_1": 2,
      "recall_at_5": 3,
      "total": 3,
      "per_question": [
        {
          "question_id": "q001",
          "gold_doc_id": "gold/russia-ukraine-conflict_2022-08-05T12_31_57Z",
          "retrieved_top_1": "hugginglearners/russia-ukraine-conflict-articles__2022-07-27T00_29_03Z",
          "recall_at_1": 0,
          "recall_at_5": 1
        },
        {
          "question_id": "q002",
          "gold_doc_id": "gold/the-world-factbook-by-cia__Mali_history",
          "retrieved_top_1": "gold/the-world-factbook-by-cia__Mali_history",
          "recall_at_1": 1,
          "recall_at_5": 1
        },
        {
          "question_id": "q003",
          "gold_doc_id": "gold/cnn_dailymail__c977693b405a89cec98e53b05199e608fd6adeca",
          "retrieved_top_1": "gold/cnn_dailymail__c977693b405a89cec98e53b05199e608fd6adeca",
          "recall_at_1": 1,
          "recall_at_5": 1
        }
      ],
      "recall_at_1_pct": 66.66666666666666,
      "recall_at_5_pct": 100.0
    },
    "citation": {
      "total_questions": 3,
      "questions_with_evidence": 3,
      "precision_sum": 1.3333333333333333,
      "recall_sum": 0.8846153846153846,
      "f1_sum": 0.9555555555555556,
      "per_question": [
        {
          "question_id": "q001",
          "gold_doc_id": "gold/russia-ukraine-conflict_2022-08-05T12_31_57Z",
          "gold_sentences": [
            "S14",
            "S15"
          ],
          "pred_sentences": [
            "S92",
            "S93"
          ],
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0
        },
        {
          "question_id": "q002",
          "gold_doc_id": "gold/the-world-factbook-by-cia__Mali_history",
          "gold_sentences": [
            "S12",
            "S15",
            "S16",
            "S17",
            "S18",
            "S19",
            "S20",
            "S21",
            "S25",
            "S26",
            "S27",
            "S28",
            "S31"
          ],
          "pred_sentences": [
            "S15",
            "S16",
            "S17",
            "S18",
            "S19"
          ],
          "precision": 1.0,
          "recall": 0.38461538461538464,
          "f1": 0.5555555555555556
        },
        {
          "question_id": "q003",
          "gold_doc_id": "gold/cnn_dailymail__c977693b405a89cec98e53b05199e608fd6adeca",
          "gold_sentences": [
            "S15",
            "S4"
          ],
          "pred_sentences": [
            "S2",
            "S3",
            "S4"
          ],
          "precision": 0.3333333333333333,
          "recall": 0.5,
          "f1": 0.4
        }
      ],
      "avg_precision": 0.4444444444444444,
      "avg_recall": 0.2948717948717949,
      "avg_f1": 0.31851851851851853
    },
    "verification": {
      "total_predictions": 3,
      "supported": 2,
      "partial": 1,
      "not_supported": 0,
      "na": 0,
      "total_attempts": 5,
      "per_question": [
        {
          "question_id": "q001",
          "verification_status": "SUPPORTED",
          "verification_attempts": 1
        },
        {
          "question_id": "q002",
          "verification_status": "SUPPORTED",
          "verification_attempts": 1
        },
        {
          "question_id": "q003",
          "verification_status": "PARTIAL",
          "verification_attempts": 3
        }
      ],
      "supported_pct": 66.66666666666666,
      "partial_pct": 33.33333333333333,
      "not_supported_pct": 0.0,
      "avg_attempts": 1.6666666666666667
    },
    "llm_judge": {
      "total_questions": 3,
      "questions_with_rubrics": 3,
      "answer_score_sum": 8,
      "evidence_score_sum": 0.8661202185792349,
      "combined_score_sum": 1.2330601092896174,
      "lambda_weight": 0.5,
      "per_question": [
        {
          "question_id": "q001",
          "answer_score_raw": 4,
          "answer_score_normalized": 0.8,
          "answer_rationale": "The model answer includes the 15% reduction figure, mentions the winter timeframe, and states that the plan includes opt-outs for certain countries, which aligns with the gold answer's mention of exemptions. However, it does not explicitly state that the plan takes effect next week or that Hungary opposed the decision.",
          "evidence_score": 0.0,
          "combined_score": 0.4,
          "gold_sentences": [
            "S14",
            "S15"
          ],
          "pred_sentences": [
            "S92",
            "S93"
          ]
        },
        {
          "question_id": "q002",
          "answer_score_raw": 3,
          "answer_score_normalized": 0.6,
          "answer_rationale": "The model answer includes some correct events such as the 2012 rebellion, 2013 French-led intervention, and 2015 peace accord failures, but lacks causal links or chronology, and omits major events like the 1991 transition and 2020-2021 coups.",
          "evidence_score": 0.366120218579235,
          "combined_score": 0.4830601092896175,
          "gold_sentences": [
            "S12",
            "S15",
            "S16",
            "S17",
            "S18",
            "S19",
            "S20",
            "S21",
            "S25",
            "S26",
            "S27",
            "S28",
            "S31"
          ],
          "pred_sentences": [
            "S15",
            "S16",
            "S17",
            "S18",
            "S19"
          ]
        },
        {
          "question_id": "q003",
          "answer_score_raw": 1,
          "answer_score_normalized": 0.2,
          "answer_rationale": "The model answer contains fabricated details, such as the vote being successful and the mention of Gordon Johndroe, which is not present in the gold answer.",
          "evidence_score": 0.5,
          "combined_score": 0.35,
          "gold_sentences": [
            "S15",
            "S4"
          ],
          "pred_sentences": [
            "S2",
            "S3",
            "S4"
          ]
        }
      ],
      "avg_answer_score": 2.6666666666666665,
      "avg_evidence_score": 0.2887067395264116,
      "avg_combined_score": 0.41102003642987245
    }
  }
}